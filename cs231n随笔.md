# <center>CS231n随笔 </center>
### 1. 神经网络(Neural Network)

1. Multiclass Support Vector Machine Loss(多类支持向量损失)。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界△。损失函数为：
$$
L=\frac{1}{N}\sum_iL_i+λR(W)\quad   
L_i=\sum_{i≠y_i}max(0,s_j-s_{y_i}+\Delta)
$$
其中$y_i$是分类得分最高的那个类，$s_{y_i}$代表分类最高的那个类的分数，$s_j$代表分类错误的类的分数。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$，这样SVM损失函数为0。


2. Softmax分类器使用的是交叉熵损失(cross-entropy loss)：
$$
\begin{align*}
H(p,q)=-\sum_xp(x)log(q(x))
\end{align*}
$$
其中$p(x)$是真实的分布，$q(x)$是估计的分布。 **可能性分布的集中或者离散程度是由正则化参数λ直接决定的，若是λ更大，权重W就会被惩罚的更多，然后他的权重数值就会更小，这样算出来的分数也会更小。**区分这两种分类器:  
![](/home/gwh/Pictures/For_notes/cs231n/Soft_SVM.png)


3. 在使用ReLU激活函数时，使用**w = np.random.randn(n)*sqrt(2.0/n)**来进行权重的初始化。若权重全部初始化为**0**，神经元之间失去了不对称的源头。[参考神经网络笔记２。](https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit)
4. 正则化是一个常用的方法防止神经网络过拟合，可以通过惩罚目标函数中所有参数的平方(L2)将其实现。

## 1.多任务学习
### 1.1交替训练
![交替训练](/home/gwh/Desktop/tensorflow/炼数成金Tensorflow代码/md_pictures/交替训练.png)
不同的任务，不同的数据集。例如英语翻译为中文、英语翻译为日文。
### 1.2联合训练
![联合训练](/home/gwh/Desktop/tensorflow/炼数成金Tensorflow代码/md_pictures/联合训练.png)
5位验证码相当于有5个Task，然后5个loss相加，传入同一个优化器。

## 2. 迁移学习


​	在实践中，由于数据集不够大，很少有人从头开始训练网络。常见的做法是使用预训练的网络（例如在ImageNet上训练的分类1000类的网络）来重新fine-tuning（也叫微调），或者当做特征提取器。

**是否使用迁移学习的两个要点：新数据集的大小、新数据和原始数据集的相似程度。**

**网络前几层学到的是通用特征，后面几层学到的是与类别相关的特征。**

​	这里有使用的四个场景：
1. 
   新数据集比较小且和原数据集相似。因为新数据集比较小，如果fine-tune可能会过拟合；又因为新旧数据集类似，我们期望他们高层特征类似，可以使用预训练网络当做特征提取器，用提取的特征训练线性分类器。
2. 
   新数据集大且和原数据集相似。因为新数据集足够大，可以fine-tune整个网络。
3. 
   新数据集小且和原数据集不相似。新数据集小，最好不要fine-tune，和原数据集不类似，最好也不使用高层特征。这时可是使用前面层的特征来训练SVM分类器。
4. 
   新数据集大且和原数据集不相似。因为新数据集足够大，可以重新训练。但是实践中fine-tune预训练模型还是有益的。新数据集足够大，可以fine-tine整个网络。

​    与重新训练相比，fine-tune要使用**更小的学习率**。因为训练好的网络模型权重已经平滑，我们不希望太快扭曲（distort）它们（尤其是当随机初始化线性分类器来**分类预训练模型提取的特征时**）。